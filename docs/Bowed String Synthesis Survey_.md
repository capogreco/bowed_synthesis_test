# **Synthesizing Realistic Bowed String Instruments in the Web Audio API: A Survey of State-of-the-Art DSP Methods for AudioWorklet**

## **I. Introduction: The Pursuit of Realistic Bowed String Synthesis in Web Audio**

### **The Challenge of Bowed String Realism**

The auditory character of bowed string instruments—violin, viola, cello, and double bass—is renowned for its expressive depth and timbral complexity. This richness, however, presents a formidable challenge for digital sound synthesis. The interaction between the bow and string is a highly nonlinear process, giving rise to a complex vibrational behavior that includes a characteristic slip-stick motion responsible for the instrument's primary tone. Beyond this fundamental mechanism, the sound is further shaped by the subtle nuances of the performer's gestures, such as variations in bow pressure, speed, and position, as well as left-hand techniques like vibrato and portamento. The instrument's body also plays a crucial role, acting as a resonator that filters and radiates the string's vibrations, imparting the unique timbral signature associated with each member of the string family. Capturing these interacting physical phenomena and their acoustic consequences to a degree that achieves perceived realism is a central goal in advanced sound synthesis.

### **Web Audio and AudioWorklet as a New Frontier**

The Web Audio API has emerged as a potent platform for sophisticated audio processing directly within web browsers, democratizing access to audio manipulation tools.1 A pivotal advancement within this API is the AudioWorklet interface.1 AudioWorklet allows developers to implement custom Digital Signal Processing (DSP) algorithms that execute in a dedicated thread, separate from the main browser thread responsible for user interface rendering and other tasks. This architectural separation is designed to provide low-latency audio processing, a critical requirement for real-time sound synthesis and effects. The introduction of AudioWorklet has thus opened new avenues for creating complex and responsive musical instruments and audio applications on the web, approaching capabilities previously restricted to native desktop environments.  
However, the pursuit of highly realistic bowed string sounds pushes the boundaries of typical web-based audio applications. While AudioWorklet furnishes the necessary tools for custom DSP, the inherent complexity of bowed string physics demands sophisticated algorithms that are often computationally intensive.4 Physical models that accurately capture nonlinear friction, two-dimensional string motion, and gestural control involve solving complex systems of equations at each audio sample or block. Deploying such models within the constraints of a web browser—even with the performance enhancements of AudioWorklet and technologies like WebAssembly (Wasm)—presents a significant engineering challenge. There exists a fundamental tension between the desire for maximal acoustic realism and the practical limitations of the web environment, including CPU resources, memory constraints, and the single-threaded nature of JavaScript for UI and primary logic. This report must therefore not only survey advanced synthesis methods but also critically evaluate their feasibility and practicality for AudioWorklet implementation.

### **Report Objectives and Scope**

This report aims to provide a comprehensive survey of state-of-the-art DSP methods for synthesizing the sounds of violin, viola, cello, and double bass, with a particular emphasis on their implementation within the AudioWorklet framework. The objective is to identify techniques that offer the highest potential for realism while considering the unique capabilities and constraints of web-based, real-time audio processing. The scope encompasses an examination of physical modeling approaches, including Finite Difference Time Domain (FDTD) methods, waveguide synthesis, and modal synthesis, as well as emerging hybrid and neural synthesis techniques. Critical aspects such as computational efficiency, control parameterization for expressive performance, and strategies for differentiating the timbral characteristics of the individual instruments will be discussed. Furthermore, the report will delve into the specifics of the AudioWorklet environment, including performance optimization strategies and the role of WebAssembly, to provide a practical guide for developers seeking to achieve high-fidelity bowed string synthesis on the web.

## **II. The AudioWorklet Environment: A Platform for High-Performance DSP**

The AudioWorklet interface is a cornerstone of the Web Audio API, designed to enable developers to create custom audio processing modules that execute with low latency. Understanding its architecture, advantages, and limitations is crucial for implementing sophisticated DSP for bowed string synthesis.

### **A. Core Concepts and Architecture**

The AudioWorklet system involves two primary components: the AudioWorkletNode and the AudioWorkletProcessor.1  
An AudioWorkletNode is an AudioNode that integrates custom audio processing logic into the Web Audio API's graph. It acts as the interface between the main browser thread and the audio processing code.  
The AudioWorkletProcessor is a JavaScript class where the actual audio processing logic resides. Code within an AudioWorkletProcessor runs in a separate global execution context known as the AudioWorkletGlobalScope. This scope operates on a dedicated audio rendering thread, distinct from the main browser thread that handles user interface updates and other JavaScript execution.1 This separation is fundamental to AudioWorklet's low-latency capabilities, as it insulates audio processing from potential blocking operations or garbage collection pauses on the main thread.  
To use a custom processor, its definition must first be loaded into the AudioContext using the audioContext.audioWorklet.addModule() method.1 This method takes the URL of a JavaScript file containing the AudioWorkletProcessor class definition(s). Once the module is added, instances of AudioWorkletNode can be created, linking to a registered AudioWorkletProcessor.  
The core of the audio processing occurs within the process(inputs, outputs, parameters) method of the AudioWorkletProcessor.2 This method is called repeatedly by the audio system at regular intervals to process blocks of audio data. The inputs and outputs arguments are arrays of Float32Arrays, representing the incoming and outgoing audio samples for each channel. Typically, the audio system processes data in blocks of 128 frames.2 The parameters argument provides access to AudioParam values, allowing for sample-accurate control of synthesis parameters. The process() method must execute synchronously, filling the output buffers based on the input buffers and current parameter values within a strict time budget.

### **B. Advantages for Low-Latency Synthesis**

The primary advantage of AudioWorklet over its predecessor, the ScriptProcessorNode, is significantly lower and more predictable latency.1 The ScriptProcessorNode executed its processing callback on the main browser thread, making it susceptible to high latency and audio glitches if the main thread was busy.1 Furthermore, ScriptProcessorNode often involved buffer sizes that could lead to latencies from 128 to 2048 samples, or roughly 2.9 ms to 42.6 ms at a 48 kHz sample rate.  
AudioWorklet, by running on a dedicated audio rendering thread, minimizes the overhead associated with context switching to the main thread and avoids interference from other browser tasks.1 This design allows for more consistent and lower-latency audio processing, which is essential for responsive virtual instruments and real-time effects.

### **C. Critical Performance Considerations for Real-Time DSP**

Achieving reliable real-time performance in AudioWorklet requires careful attention to several factors:

* **Buffer Size and Latency:** The Web Audio API typically processes audio in blocks of 128 sample frames.2 At a standard sample rate of 48 kHz, this means the process() method must complete all its computations for 128 frames within approximately 128/48000 seconds≈2.67 milliseconds.2 Exceeding this deadline will result in audio stuttering or glitches, as the system will not have new audio data ready for output.2 This tight time budget dictates the complexity of algorithms that can be reliably executed.  
* **Avoiding Garbage Collection (GC):** JavaScript is a garbage-collected language. If memory is allocated and deallocated frequently within the process() method, the garbage collector may run, introducing unpredictable pauses that can disrupt audio continuity.1 To mitigate this, developers should:  
  * Minimize or avoid object creation within process().  
  * Use Float32Array and other TypedArrays for audio data and reuse them across calls.  
  * Pre-allocate any necessary buffers or objects during the processor's constructor or initialization phase. Firefox's implementation of AudioWorklet is noted for its design that helps developers avoid triggering garbage collection on the real-time audio processing thread if careful coding practices are followed.2  
* **Efficient Algorithms and Arithmetic:** The DSP algorithms themselves must be highly efficient. This involves:  
  * Prioritizing simple arithmetic operations. For instance, multiplication is generally faster than exponentiation or trigonometric functions if these can be avoided or precomputed.1  
  * Choosing algorithms with low computational complexity.  
  * The overarching principle is to "avoid anything that can result in non-deterministic computation time".2 This includes avoiding operations like dynamic memory allocation, complex conditional logic that significantly varies execution time, or I/O operations within the process() method.  
* **Memory Management:** Beyond GC, general memory management is crucial. When using WebAssembly, ensure that memory is handled efficiently. Communication with the main thread via MessagePort (for sending control data or custom messages) involves data copying, so the complexity of these data structures should be minimized to reduce allocation overhead and copying time.2 When passing large ArrayBuffers, such as between a Web Worker and an AudioWorklet script, transferring ownership rather than copying can significantly improve performance and avoid memory allocation on the real-time thread.2

### **D. Harnessing WebAssembly (Wasm) for Computationally Intensive DSP**

For complex synthesis algorithms, such as those required for realistic physical modeling, JavaScript performance may be insufficient. WebAssembly (Wasm) provides a way to run code written in languages like C, C++, or Rust at near-native speed in the browser.1

* **Wasm for Performance:** Wasm is a binary instruction format that serves as a compilation target for high-level languages. It is designed for efficient execution and is a natural fit for CPU-intensive DSP tasks.1  
* **Emscripten and AudioWorklet Integration:** The Emscripten toolchain is commonly used to compile C/C++ code to Wasm and provides integration with AudioWorklet.8 Developers can implement their AudioWorkletProcessor logic in C/C++ and compile it into a Wasm module. The Emscripten Wasm Audio Worklets system runtime is specifically designed to prevent the generation of temporary JavaScript VM garbage during processing, thereby eliminating the risk of GC pauses originating from the Wasm module's interaction with the system.8  
* **Execution Context:** Wasm AudioWorkletProcessors execute their processing callback (a C/C++ function pointer) on the same dedicated audio processing thread used by JavaScript-based AudioWorkletProcessors.8 This thread typically operates with real-time processing priority, subject to the browser's and operating system's scheduling policies.  
* **Wasm Considerations:** The process() method in a Wasm-based AudioWorkletProcessor is still callback-driven and operates on 128-sample blocks. Therefore, the Wasm code must also be non-blocking and complete its execution within the tight time budget.8 Spinning custom loops or performing blocking operations is not permissible.

The use of Wasm is particularly relevant for physical modeling of bowed strings, as these models often involve extensive numerical computations that benefit significantly from the performance of compiled code. However, simply porting a desktop-grade native physical model to Wasm may not be sufficient. The algorithm must be structured to work efficiently within the block-based, callback-driven architecture of AudioWorklet. If a Wasm module's computation for a single 128-sample block exceeds the \~2.67 ms deadline, audio glitches will occur irrespective of Wasm's raw execution speed. This implies that the algorithm itself might need re-architecting for this environment, perhaps by breaking down computations across multiple blocks or offloading less time-critical parts to Web Workers if the overall latency budget allows.2

### **E. Addressing Current Limitations: The Real-Time Thread Priority Conundrum**

A significant challenge for achieving robust, professional-grade audio synthesis in AudioWorklet relates to the priority of its execution thread. While AudioWorklet is designed to run on a separate thread for low latency, the actual scheduling priority of this thread can vary between browsers and platforms, impacting performance under heavy system load.

* **The Chromium Issue:** A long-standing issue in Chromium-based browsers (e.g., Google Chrome, Microsoft Edge) is that the WebAudio rendering thread, which hosts AudioWorkletProcessors, often runs with "DISPLAY" priority rather than a true real-time priority.9 This means that other tasks, particularly graphics rendering or demanding processes outside the browser, can preempt the audio thread, leading to audible glitches, crackling, or dropouts, even if the AudioWorklet code itself is highly optimized. Users have reported that even passing input directly to output without any processing can result in dropped frames under high CPU load.9  
* **Impact on "Serious Audio Processing":** Many audio professionals and developers argue that "serious audio processing REQUIRES a real-time thread" or a comparable equivalent to ensure reliable, glitch-free operation.9 The lack of guaranteed real-time priority in some major browsers can undermine the suitability of AudioWorklet for demanding, production-ready applications where audio integrity is paramount. As of late 2024, this issue in Chromium was reportedly still under discussion, indicating its persistent nature.9  
* **Firefox's Approach:** In contrast, Mozilla Firefox's implementation of AudioWorklet aims to run the audio processing code on a dedicated real-time system thread.2 This approach generally offers better isolation from other system activities and can lead to more stable audio performance, especially for complex DSP tasks.

This discrepancy in thread priority implementation means that developers cannot universally assume guaranteed real-time behavior for their AudioWorklet-based applications. The performance and reliability of a complex bowed string synthesizer could vary significantly depending on the user's browser and the overall system load. This may necessitate more defensive programming, potentially favoring simpler algorithms, or relying heavily on Wasm to maximize the computational work done per audio block to stay within budget even with occasional preemptions. For the most demanding synthesis tasks, Firefox might currently offer a more consistently stable environment if this threading priority difference persists.

### **F. Emerging Capabilities for Enhanced Performance**

Several web platform features, when combined with AudioWorklet, promise to further enhance performance and capabilities for complex audio synthesis:

* **SharedArrayBuffer (SAB):** SharedArrayBuffer allows for the creation of memory that can be shared between different JavaScript threads (e.g., the main thread, Web Workers, and AudioWorklet threads).2 This enables lock-free programming paradigms, where threads can communicate and synchronize data without traditional locking mechanisms that can introduce non-determinism. For AudioWorklet, SAB is particularly useful for efficiently sharing large amounts of data (e.g., control parameters, lookup tables, or even parts of the model state) between the main control logic and the real-time audio processing code. Its availability was initially impacted by security concerns (Spectre/Meltdown) but is now generally accessible in secure contexts that implement appropriate cross-origin isolation policies.  
* **WebAssembly SIMD (Single Instruction, Multiple Data):** Wasm SIMD allows a single instruction to operate on multiple data elements simultaneously.2 Many DSP algorithms, including those used in physical modeling (e.g., vector and matrix operations in FDTD solvers), can benefit significantly from SIMD parallelization, leading to substantial speed-ups.  
  * **Browser Support Status (2024-2025):** The 128-bit fixed-width Wasm SIMD specification is standardized (Phase 5\) and widely supported in modern versions of Chrome (since v91), Firefox (since v89), Safari (since v16.4), and Node.js (since v16.4).10 The "Relaxed SIMD" proposal, which aims to take advantage of additional hardware-specific SIMD instructions, has also reached Phase 5 (standardized) and is expected to be fully enabled (without flags) in Firefox and Safari during 2025, having already been live in Chrome.10

The combination of AudioWorklet, Wasm, SAB, and Wasm SIMD provides a powerful toolkit for tackling computationally demanding real-time audio synthesis on the web.  
The following table summarizes key optimization strategies for developing high-performance DSP applications using AudioWorklet and WebAssembly:

| Optimization Area | Specific Technique/Consideration | Impact on Performance/Latency | Key Sources |
| :---- | :---- | :---- | :---- |
| **Memory Management (JS)** | Minimize object creation in process(); reuse TypedArrays and other objects. | Reduces GC pauses, lowers latency spikes. | 1 |
|  | Pre-allocate buffers and complex objects in constructor or on initialization. | Avoids allocation overhead during real-time processing. | 2 |
| **JavaScript Logic** | Avoid complex, non-deterministic operations; simplify conditional logic. | Ensures process() completes within budget, reduces jitter. | 2 |
|  | Limit or avoid console logging in process() during production. | Excessive logging introduces significant latency. | 1 |
| **WebAssembly (Wasm) Performance** | Compile C/C++ to Wasm with high optimization levels (e.g., \-O3, Link-Time Optimization). | Maximizes raw computation speed within Wasm. | 2 |
|  | Utilize Wasm SIMD for parallelizable DSP algorithms. | Significant speed-up for vector/matrix operations. | 2 |
|  | Ensure Wasm module is non-blocking and completes its per-block task quickly. | Critical for meeting AudioWorklet's 128-sample deadline. | 8 |
| **JS-Wasm Interoperability** | Minimize data copying between JS and Wasm; transfer ownership of ArrayBuffers when possible. | Reduces overhead of calling Wasm and memory pressure. | 2 |
|  | Use SharedArrayBuffer for efficient, lock-free sharing of control data or larger state. | Reduces communication latency and avoids main-thread blocking. | 2 |
| **Algorithmic Design for AudioWorklet** | Ensure algorithms are designed for block-based processing (typically 128 frames). | Prevents audio glitches due to missed deadlines. | 2 |
|  | Break down overly complex computations across multiple process() calls if latency permits. | Manages computational load within the per-block budget. | 2 |
|  | For control parameters, use AudioParam for sample-accurate automation where feasible. | Efficient, built-in mechanism for parameter updates. | 1 |
|  | For other control data, use MessagePort with simple data structures. | Minimizes overhead of custom message passing. | 1 |

These strategies are essential for pushing the boundaries of what is achievable in web-based audio synthesis, particularly for demanding tasks like realistic bowed string emulation.

## **III. State-of-the-Art Physical Modeling Techniques for Bowed Strings**

Physical modeling aims to synthesize sound by simulating the underlying physical laws governing sound production in an instrument. For bowed strings, this involves modeling the vibrating string, the complex friction interaction with the bow, and often the influence of the instrument body.

### **A. Foundational Principles**

* **The Bow-String Interaction:** The characteristic sound of a bowed string arises primarily from the nonlinear interaction between the bow hair and the string. This interaction is governed by friction. When the bow is drawn across the string, static friction initially causes the string to be displaced with the bow. As the displacement increases, the restoring force in the string also increases until it overcomes the maximum static friction force. At this point, the string slips back rapidly in the direction opposite to the bow's motion.12 During this slip phase, the friction between the bow and string is dynamic, which is typically less than static friction, allowing the string to overshoot its rest position. The bow then recaptures the string, and the cycle repeats. This periodic stick-slip behavior is known as Helmholtz motion and results in a waveform that is approximately sawtooth-like, rich in harmonics.12 The precise nature of this interaction is highly dependent on several "gestural" parameters controlled by the player:  
  * **Bow Pressure (Normal Force):** The downward force exerted by the bow on the string.  
  * **Bow Velocity (Tangential Speed):** The speed at which the bow is drawn across the string.  
  * **Bow Position (Contact Point):** The distance of the bow from the bridge. The properties of the bow itself, such as the tension and material of the horsehair, also influence the sound.4  
* **Two-Polarisation String Models:** For a high degree of realism, it is crucial to model the string's motion in two perpendicular planes, often referred to as vertical (or normal) and horizontal (or tangential) polarisations relative to the bow's motion.4 The motion in the vertical plane (perpendicular to the fingerboard and parallel to the bow pressure) involves collision-like interactions with the bow, finger, and fingerboard. These normal forces, in turn, modulate the tangential friction forces in the horizontal plane (parallel to the fingerboard and the direction of bow movement). This coupling between the two polarisations leads to complex, three-dimensional string trajectories and is essential for capturing many subtle aspects of bowed string sound, including certain timbral characteristics and the initiation of Helmholtz motion.  
* **Modeling Damping and Losses:** Real strings are not ideal lossless vibrators. Energy is dissipated through various mechanisms, including internal friction within the string material, air resistance, and losses at the points where the string is terminated (e.g., bridge, nut). This damping is generally frequency-dependent, with higher frequencies often decaying more rapidly than lower ones. Accurately modeling these frequency-dependent losses is perceptually important for the naturalness of the synthesized sound, particularly for the decay characteristics of notes.4 The NESS (Next Generation Sound Synthesis) project, for instance, has focused on developing accurate time-domain models for frequency-dependent damping in strings.4

### **B. Finite Difference Time Domain (FDTD) Methods**

FDTD methods are a powerful class of numerical techniques for solving partial differential equations (PDEs) by discretizing them in both time and space. They are widely used in physical modeling sound synthesis for simulating the behavior of vibrating objects like strings, membranes, and plates.

* **Core Principles:** In FDTD, the continuous domain of the string (length and time) is replaced by a discrete grid of points. The PDEs describing the string's motion are approximated by finite difference equations, which relate the displacement (or other state variables) at a grid point to the values at neighboring points in space and previous points in time. By iteratively solving these difference equations, the evolution of the string's vibration can be simulated.  
* Application to String Equations: A common PDE for a stiff string, considering transverse displacement u(x,t) at position x and time t, can be written as:  
  $$ \\rho A \\frac{\\partial^2 u}{\\partial t^2} \= T \\frac{\\partial^2 u}{\\partial x^2} \- EI \\frac{\\partial^4 u}{\\partial x^4} \- d\_0 \\frac{\\partial u}{\\partial t} \+ d\_1 \\frac{\\partial^3 u}{\\partial x^2 \\partial t} \+ f\_{ext}(x,t) $$  
  where ρ is density, A is cross-sectional area, T is tension, E is Young's modulus, I is the area moment of inertia (related to stiffness), d0​ and d1​ are damping coefficients, and fext​(x,t) represents external forces, such as those from the bow or fingers.5 FDTD schemes are developed to approximate the various partial derivatives in this equation.  
* **Advanced Friction Models:** The accuracy of bowed string synthesis heavily relies on the sophistication of the friction model used for fext​(x,t) at the bow contact point.  
  * **Force-Velocity Friction Curves:** Simpler models define the friction force as a direct function of the relative velocity between the bow and the string.5 These curves typically exhibit a region of static friction (stiction) at zero or very low relative velocities and a region of dynamic (kinetic) friction at higher velocities, often with a negative slope (friction decreases as slip speed increases) which is crucial for Helmholtz motion.  
  * **Elasto-Plastic Friction Models:** These are considered state-of-the-art for capturing more detailed and realistic friction phenomena.13 Elasto-plastic models often conceptualize the contact interface as a collection of microscopic "bristles" or asperities that can deform elastically up to a certain point (breakaway displacement) and then yield or slip. This introduces an internal state variable (e.g., average bristle deflection) into the friction model, leading to hysteretic behavior (where the friction force depends not only on the current relative velocity but also on its history) and a more accurate representation of phenomena like pre-sliding displacement and varying static friction. Willemsen, Bilbao, and Serafin have detailed the real-time FDTD implementation of a stiff string coupled with an elasto-plastic friction model.16  
* **Nonlinear Contact Models (Finger/Fingerboard):** In the normal polarisation (perpendicular to bowing), the interactions of the string with the player's finger and the instrument's fingerboard are modeled as collisions. The Hunt and Crossley collision model, which includes both a stiffness term and a dissipative (damping) term dependent on the penetration depth and velocity, is often used to provide a smooth and physically plausible representation of these contacts.4  
* **Ensuring Numerical Stability:** Explicit FDTD schemes, while straightforward to implement, can suffer from numerical instability, where errors grow unboundedly, especially when strong nonlinearities (like friction or hard collisions) are present. This is a critical concern for real-time synthesis.  
  * **Energy-Balanced Schemes:** A robust approach to ensure stability is to design the finite difference scheme such that it preserves a discrete analogue of the physical system's energy.5 If the discrete energy can be shown to be non-increasing (in the absence of external energy input), the scheme is guaranteed to be stable. Desvages' work on two-polarisation bowed string models heavily emphasizes the use of globally energy-balanced schemes to handle the highly nonlinear conditions arising from contact and friction forces.5  
  * **Passivity:** Closely related to energy balance, passivity ensures that the numerical system does not spontaneously generate energy, which is a common cause of instability.  
* **Computational Cost and Real-Time Feasibility in AudioWorklet:** FDTD methods are inherently computationally intensive because the state of many spatial grid points must be updated at each time step (typically at the audio sampling rate).6 The number of grid points depends on the string length and the highest frequency to be accurately modeled.  
  * Willemsen et al. demonstrated that a real-time FDTD simulation of a single stiff string with an elasto-plastic friction model could be achieved with less than 6% CPU usage on a single core of a modern processor in a native C++ implementation.14 While this is a promising benchmark, translating this performance to Wasm running within AudioWorklet requires careful optimization and consideration of the browser environment's overheads.  
  * The nonlinear equations arising from friction and contact models often need to be solved implicitly at each time step. This typically involves iterative solvers (e.g., Newton-Raphson), which can be computationally expensive and have a variable number of iterations, posing a challenge for the strict real-time deadlines of AudioWorklet. Therefore, there is a strong drive towards non-iterative or highly efficient, bounded-iteration solvers.5

The combination of stability and efficiency is paramount for advanced physical modeling. A numerically stable scheme that is too slow due to iterative nonlinear solvers is not viable for AudioWorklet. Conversely, a fast but unstable scheme is unusable. The cutting edge of research focuses on methods that are both provably stable (often by design through energy-conserving principles) and computationally efficient, frequently by employing non-iterative solution techniques. This synergy is critical for realizing the goal of high-realism physical modeling in constrained environments.

### **C. Waveguide Synthesis**

Digital waveguide synthesis is another established physical modeling technique, particularly known for its computational efficiency in simulating one-dimensional wave propagation, such as in strings or acoustic tubes.

* **Digital Waveguides:** The core idea, formalized by Julius O. Smith and others, is to model the forward and backward traveling waves along a string (or other 1D medium) using digital delay lines.6 Terminations, reflections, and transmission at boundaries or interaction points (like the bow or a finger) are modeled by digital filters and scattering junctions. For simple linear, time-invariant (LTI) systems, this approach can be extremely efficient as losses and dispersion can often be "lumped" into a few filter elements within the waveguide loop.6  
* **Application to Bowed Strings:** Early and influential models of bowed strings, such as those by Smith in 1982 22, utilized the waveguide framework. In these models, the bow-string interaction (friction) is typically implemented as a nonlinear element that reads the incoming wave components from the string segments on either side of the bow and injects outgoing wave components based on a friction curve and the bow's parameters (pressure, velocity). String terminations (e.g., at the bridge and nut) are modeled as reflection filters.  
* **Limitations for Complex Nonlinearities:** While standard digital waveguides are highly efficient for LTI systems and can incorporate localized nonlinearities, they can face challenges when dealing with interactions that are distributed along the string or involve complex, time-varying nonlinearities and coupling between multiple vibrational modes or polarisations, as found in highly realistic bowed string models.6 Extending waveguides to accurately capture all the nuances of, for example, two-polarisation motion with distributed fingerboard contact and advanced friction models can become complex and may negate some of an idealized waveguide's efficiency benefits.  
* **Variants (e.g., Banded Waveguides):** Banded waveguide synthesis is a variant designed to model dispersive wave propagation (where wave speed depends on frequency) by splitting the signal into frequency bands, each modeled by a band-limited waveguide.23 While primarily developed for instruments with strong inharmonicity like bells and bars 23, the principle of handling dispersion in frequency bands might have some relevance, although FDTD and modal methods are more commonly used for the dispersive effects (due to stiffness) in strings.

### **D. Modal Synthesis**

Modal synthesis represents the vibration of an object as a sum of its discrete resonant modes. Each mode has a characteristic frequency, amplitude, and decay rate (damping).

* **Core Principles:** The sound of an instrument is decomposed into the contributions of its natural modes of vibration. For a string, these modes correspond to the various harmonics (for an ideal string) or slightly inharmonic partials (for a stiff string). The synthesis process involves exciting these modes and letting their amplitudes evolve over time according to their damping characteristics.22  
* **Advantages:** Modal synthesis can be computationally efficient, especially if the sound can be well-represented by a relatively small number of perceptually important modes. It offers intuitive control over the spectral components of the sound, as each mode's parameters (frequency, amplitude, damping) can often be directly manipulated.  
* **Challenges:** A primary challenge in modal synthesis for interactively played instruments like bowed strings is accurately modeling the nonlinear excitation and interaction mechanisms (like friction) that couple the modes. Calculating the modal parameters (frequencies and shapes) for complex geometries or boundary conditions can also be difficult, though for strings, these are often well-defined.  
* **Recent Advances: Non-Iterative Modal Solvers:** A significant recent development that enhances the viability of modal synthesis for complex nonlinear interactions is the application of non-iterative numerical solution techniques.  
  * Russo, Ducceschi, and Bilbao (DAFx 2022\) presented an efficient method for simulating the bowed string in modal form.24 Their approach involves:  
    1. Expressing the string's PDE as a system of ODEs by performing a modal decomposition (projecting the string's motion onto its mode shapes).  
    2. Applying a specialized non-iterative time-stepping procedure (adapted from methods for stiff nonlinear ODEs in audio circuit simulation) to solve this system of modal ODEs.  
  * The key to the efficiency of this non-iterative modal solver lies in the structure of the linear system that needs to be solved at each time step. This system matrix can be expressed as a block-diagonal matrix plus a rank-1 perturbation. The inverse of such a matrix can be computed very efficiently using the Sherman-Morrison formula.24 The block-diagonal part arises from the uncoupled linear modal dynamics, and the rank-1 perturbation incorporates the localized nonlinear bow-string interaction.  
  * This method was reported to achieve faster-than-real-time performance for typical musical strings in a Matlab implementation, even with nonlinear bow interaction.24 This makes it a very strong candidate for implementation in AudioWorklet, as it combines the potential for accurate nonlinear modeling with predictable and low computational cost per time step.

### **E. Comparative Analysis of FDTD, Waveguide, and Modal Approaches for Web Audio**

The choice of synthesis technique for AudioWorklet involves balancing realism, computational complexity, and control capabilities.

* **Realism vs. Complexity:**  
  * **FDTD:** Methods, particularly those incorporating two-polarisation models, advanced elasto-plastic friction, and nonlinear contact models (e.g., Desvages 5, Willemsen et al. 16), offer the highest potential for capturing the detailed physics and thus achieving maximal realism. However, they are also the most computationally complex and challenging to implement stably and efficiently.  
  * **Digital Waveguides:** These are generally the most computationally efficient for basic string modeling.6 However, achieving the highest levels of realism, especially for complex nonlinear interactions and subtle gestural nuances, can be difficult without significant extensions that may compromise their inherent efficiency.  
  * **Modal Synthesis:** This approach offers a good balance. With a sufficient number of modes, it can produce realistic string sounds. The main historical challenge was modeling nonlinear interactions. However, recent advances like non-iterative modal solvers 24 significantly improve its capability to handle nonlinearities like bow friction efficiently, making it highly competitive.  
* **Suitability for AudioWorklet:**  
  * **FDTD:** Implementing full FDTD models in AudioWorklet is a significant undertaking. It requires aggressive optimization, implementation in Wasm (likely with SIMD), and extremely careful management of the per-block computational budget. The reported \<6% CPU usage for a single string with elasto-plastic friction by Willemsen et al. 16 is a promising native benchmark, but its direct translation to AudioWorklet performance needs to be verified, considering browser overheads and potential threading priority issues.9 The need for efficient (preferably non-iterative) solvers for the nonlinear algebraic equations at each step is critical.  
  * **Waveguide:** Due to their general efficiency, waveguide models are often well-suited for AudioWorklet implementation, especially for less complex string sounds or when computational resources are very limited. However, achieving the "most realistic possible" sound might be constrained.  
  * **Non-Iterative Modal Synthesis:** This approach appears highly promising for AudioWorklet. The demonstrated efficiency and numerical stability of non-iterative solvers for modal systems with nonlinear bow interaction 24 make it an attractive option. The predictable, fast update per time step aligns well with AudioWorklet's strict processing deadlines.  
* **Control Parameter Richness:** Physical models, whether FDTD or modal, generally offer a rich set of physically meaningful control parameters (bow pressure, speed, position; finger position and pressure, etc.). This allows for detailed and intuitive control over the sound synthesis process, which is crucial for expressive performance. Waveguide models also offer such parameters, though their mapping to subtle nuances might be less direct in simpler implementations.

The development of modular physical modeling frameworks, where complex instruments are built by interconnecting simpler, validated components (like strings, bars, and nonlinear connections) 14, offers a pathway to manage the complexity of comprehensive instrument models. Even if all processing for a multi-string instrument ultimately occurs within a single AudioWorkletProcessor due to Web Audio API's current single audio rendering thread model for a given context 28, a modular design in C++/Wasm can lead to more maintainable, testable, and potentially better-optimized code. This approach facilitates breaking down the daunting task of modeling an entire violin, for example, into more manageable sub-problems.  
The following table provides a comparative overview of these primary physical modeling techniques in the context of AudioWorklet deployment:

| Technique | Core Principle | Key Strengths for Realism | Major Challenges for AudioWorklet | Estimated Computational Tier (Single String) | Control Parameter Richness | Key Sources |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **FDTD with Adv. Friction/Contact** | Discretizes PDEs of string motion in time & space; detailed simulation of waves. | Highest potential for physical accuracy, captures complex nonlinearities, two-polarisations. | Very high computational cost, stability with nonlinearities, requires non-iterative/efficient solvers, demanding for 128-sample budget. | High | Very High | 5 |
| **Digital Waveguide Synthesis** | Models traveling waves using delay lines and filters. | Computationally efficient for LTI systems, good for basic string sounds. | Difficulty with distributed/complex nonlinearities, may limit ultimate realism for subtle nuances. | Low to Medium | Medium to High | 6 |
| **Non-Iterative Modal Synthesis** | Represents vibration as sum of modes; uses non-iterative solver for nonlinear ODEs. | Good balance of realism & efficiency, handles nonlinear bow interaction well, fast updates. | Requires careful modal decomposition, number of modes impacts quality vs. cost. | Medium | High | 24 |

## **IV. Emerging Frontiers: Hybrid and Neural Synthesis Methods**

Beyond traditional physical modeling, hybrid approaches combining physical models with machine learning, and purely neural synthesis techniques, are emerging as promising avenues for sound generation.

### **A. Hybrid Physical Modeling and Machine Learning**

Hybrid methods seek to leverage the strengths of both physical modeling (interpretability, structural priors) and machine learning (data-driven pattern recognition, function approximation).

* **Concept:** These approaches can involve using ML to estimate parameters for a physical model, control a physical model in real-time, or even replace computationally expensive components of a physical model with more efficient neural approximations.  
* **ML for Parameter Estimation/Control:** A notable example is "Vivi the virtual violinist," a system that employs Support Vector Machines (SVMs) to control a physical model of a violin.29 The SVMs analyze the audio output generated by the physical model and adjust bowing parameters (bow force, velocity, position) and finger position to guide the synthesis towards a target sound quality, mimicking how a human student learns. The physical model itself can be treated as a "black box" by the ML controller, which learns the mapping from desired audio characteristics to control inputs.29 This demonstrates the potential for ML to manage the complex multiparametric control of physical models.  
* **Physics-Informed Neural Networks (PINNs):** PINNs are neural networks trained to solve PDEs or approximate physical systems, where the training process is constrained by known physical laws embedded as soft constraints in the loss function. While direct applications to full bowed string synthesis in AudioWorklet are still nascent, research such as Kar et al.'s investigation of a physics-informed deep learning framework for a simplified one-degree-of-freedom mass-spring system with nonlinear bow friction 3030 indicates a potential direction for learning complex components like friction models or other nonlinearities within a physically plausible framework.  
* **Potential for AudioWorklet:** Hybrid models could offer a path to more manageable complexity and computational efficiency. For instance, a computationally intensive part of a physical model (e.g., a detailed friction mechanism or body resonance) might be replaced by a trained neural network that provides a faster approximation. The primary challenge would be ensuring that the neural component is compact and fast enough for real-time execution within AudioWorklet's tight budget, and that it integrates seamlessly with the rest of the physical model.

### **B. Neural Audio Synthesis (e.g., Differentiable Digital Signal Processing \- DDSP)**

Purely neural audio synthesis techniques aim to generate audio directly from data, often using deep learning architectures. Differentiable Digital Signal Processing (DDSP) is a prominent example that has shown success in musical instrument timbre modeling.

* **Overview of DDSP:** DDSP integrates traditional DSP elements (like additive synthesis based on harmonic oscillators and filtered noise for subtractive synthesis) as learnable components within a deep neural network framework.31 Typically, an encoder network extracts control features (e.g., fundamental frequency (f0), loudness) from an input audio signal (or other conditioning input like MIDI). A decoder network then uses these features to predict the parameters for the DSP components (e.g., harmonic amplitudes, filter coefficients), which finally synthesize the output audio. The differentiability of the DSP components allows the entire system to be trained end-to-end.  
* **Application to Instrumental Timbre:** DDSP has demonstrated strong capabilities in timbre transfer, where it learns the timbral characteristics of a target instrument from a dataset of its recordings and can then "render" new melodies or audio inputs with that learned timbre.31  
* **MIDI Controllability:** Jonason, Sturm, and Thomé have shown that DDSP models can be transformed into MIDI-controllable synthesizers, where MIDI input (pitch, velocity) is used to generate the intermediate control features for the DDSP synthesis engine.31  
* **Real-Time Implementations:** Efforts have been made to implement DDSP in real-time, for example, as a VST plugin.32 This typically involves adapting the original frame-based processing of DDSP (which might process multiple frames at once for analysis and synthesis) to a sample-based or smaller block-based operation suitable for real-time audio callbacks. Optimizations such as moving the TensorFlow model inference to a separate thread and streamlining computations are often necessary.32  
* **Potential and Challenges for Realistic, Controllable Bowed String Synthesis in AudioWorklet:**  
  * **Potential:** Neural models like DDSP could potentially learn the highly complex and nuanced timbral characteristics of bowed string instruments directly from recordings, bypassing the need for explicit physical formulation of every detail.  
  * **Challenges:**  
    * **Expressive Control:** Achieving the fine-grained, continuous, and coordinated expressive control necessary for *truly realistic bowed string performance from scratch* (as opposed to timbre transfer of an existing performance or simple note-on/off triggering) is a major hurdle. Capturing dynamic variations in bow attack, pressure, speed, position, vibrato depth and rate, and transitions between playing styles (e.g., legato, staccato, spiccato) through intuitive control parameters is significantly more complex than basic pitch and loudness control. One DDSP system was noted for not incorporating note-to-note timbre dependencies, which are vital for the expressive emulation of instruments like the violin.31  
    * **Computational Load:** DDSP models, involving deep neural networks, can be computationally intensive. The real-time VST implementation mentioned encountered challenges with the computational load and achieving accurate timbre reproduction with custom-trained models.32 Deploying such models within AudioWorklet would require highly optimized network architectures and inference engines (e.g., TensorFlow Lite, ONNX Runtime for Web).  
    * **Data Requirements:** Training high-quality, controllable neural synthesizers for bowed strings demands substantial datasets. While DDSP can learn timbre from unannotated audio 31, achieving *controllable synthesis* of specific articulations and expressive nuances requires training data that pairs high-quality audio with detailed, synchronized performance parameters (e.g., continuous bow pressure, velocity, position; finger movements). Such comprehensive datasets for all four bowed string instruments across a wide range of techniques are rare and difficult to create. This data dependency is a significant bottleneck for purely data-driven approaches to achieve the "most realistic possible" and highly controllable synthesis. Physical models, while complex to formulate, derive their behavior from physical principles and do not inherently require vast datasets for their core mechanics, although calibration data can be beneficial.22  
    * **Interpretability and Control Mapping:** Physical models offer explicit, physically meaningful control parameters. Neural synthesis models like DDSP often operate on learned latent representations. Mapping intuitive musical gestures or desired articulations (e.g., "play more sul ponticello," "apply a crescendo with increasing bow speed") to the input features of a pre-trained neural model in a predictable and physically plausible way is a non-trivial research problem.31 Hybrid systems like "Vivi" 29, where ML acts as a higher-level controller for a physical model, represent one approach to bridge this gap. However, the choice of synthesis method will fundamentally impact how directly and intuitively a user can control the expressive details of the synthesized sound.

The path towards highly realistic and controllable bowed string synthesis using neural or hybrid methods in AudioWorklet is promising but involves overcoming significant challenges in expressive control, computational efficiency, and data availability.

## **V. Achieving Nuance: Expressivity and Instrument-Specific Timbre**

Achieving realistic bowed string synthesis requires not only modeling the fundamental sound production mechanisms but also capturing the vast range of expressive nuances controlled by the player and the distinct timbral characteristics of each instrument in the family.

### **A. Modeling Gestural Control in Physical Models**

Physical models excel at providing direct handles for gestural control, as their parameters often correspond to real-world actions a musician performs.

* **Bow Parameters:** The "holy trinity" of bow control consists of position, normal force (pressure), and tangential velocity.  
  * **Bow Position (xb​):** This refers to the point of contact of the bow along the string, typically measured as the distance from the bridge. Bowing closer to the bridge (sul ponticello) produces a brighter, more intense sound, rich in upper harmonics. Bowing further from the bridge, over or near the fingerboard (sul tasto), results in a softer, rounder tone with a stronger fundamental and fewer upper harmonics.4  
  * **Bow Normal Force (Fn​):** Commonly called "bowing pressure," this is the downward force applied by the bow onto the string. It is critical for establishing and maintaining stable Helmholtz motion. Insufficient pressure can lead to a "surface sound," multiple slips per cycle, or a weak, whistling tone. Excessive pressure can cause a "raucous" or "crunchy" sound, pitch flattening, or even choke the string's vibration.4  
  * **Bow Tangential Velocity (vb​):** This is the speed at which the bow is drawn across the string, perpendicular to its length. It is a primary determinant of loudness. Bow velocity interacts strongly with bow pressure and position to define regions of stable tone production, often visualized in "Schelleng diagrams" or playability-force diagrams.5 The dynamic interplay of these three parameters is crucial for articulation, dynamics (crescendo/diminuendo), and overall tonal character. Simulating realistic expression requires modeling how these parameters change over time and how they interrelate. For instance, a musician often subtly adjusts bow pressure in response to changes in bow speed or position to maintain a consistent tone or achieve a specific expressive effect. Replicating this sophisticated coordination in a synthesis model is challenging; simply exposing the raw physical parameters might not be intuitive for musical control via MIDI or other interfaces. Higher-level control mapping, potentially informed by analysis of human performance data or machine learning techniques (as seen in the "Vivi" system 29), may be necessary to translate musical intent into coordinated changes of these low-level physical parameters.  
* **Left-Hand Techniques:** The player's left hand primarily controls pitch but also contributes significantly to articulation and timbre.  
  * **Finger Position (xf​):** Stopping the string against the fingerboard at a specific position shortens its vibrating length, thereby raising the pitch. Accurate modeling of this requires simulating the interaction between the string, the finger, and the rigid fingerboard.4  
  * **Finger Force:** The pressure exerted by the stopping finger influences the clarity of the stopped note and can also affect damping.4 Insufficient finger pressure can lead to buzzing or unclear pitch.  
  * **Vibrato:** A core expressive technique, vibrato involves a periodic oscillation of the pitch of a note. It is typically produced by a rocking motion of the stopping finger, causing small, rapid changes in the string's speaking length. In physical models, this can be simulated by modulating the finger position parameter (xf​) and/or the finger pressure along the string.4 The rate and extent of vibrato are key expressive variables.  
  * **Portamento/Glissando:** These involve a smooth slide in pitch from one note to another. They are modeled by continuously varying the finger position (xf​) along the string while maintaining contact.4

### **B. Differentiating the String Family: Violin, Viola, Cello, Double Bass**

While the fundamental physics of bowing is similar across the violin family, each instrument possesses a unique timbre due to differences in its physical construction.

* **Key Physical Parameter Differences:**  
  * **String Dimensions and Properties:** The most obvious differences lie in the strings themselves. Violins have the shortest and thinnest strings, while double basses have the longest and thickest. String length, linear mass density (mass per unit length), and tension are primary determinants of the open string fundamental frequencies. These parameters also affect the string's stiffness and overall mechanical impedance, influencing its vibrational behavior and response to bowing.33  
  * **Body Resonances (Formants):** The wooden body of a string instrument acts as a complex resonator. It vibrates in response to the forces transmitted from the strings via the bridge, and it radiates sound into the surrounding air. The size, shape, wood properties, and construction details (e.g., plate thicknesses, f-hole geometry) of the body determine its natural resonant frequencies (modes). These body resonances, often referred to as formants, selectively amplify certain frequency regions and thus play a crucial role in shaping the instrument's timbre.34 Generally, larger instruments like the cello and double bass have lower-frequency body resonances compared to the violin and viola, contributing to their deeper and often richer sound.  
  * **Bridge Characteristics:** The bridge is not merely a passive support for the strings; it actively transmits vibrational energy from the strings to the body. The bridge itself has its own vibrational modes and filtering characteristics, which differ between instrument types due to variations in size, shape, and wood.  
* **Strategies for Tuning Models to Achieve Distinct Timbres:**  
  * **Direct Parameter Adjustment in String Models:** In any physical model (FDTD, waveguide, or modal), parameters defining the string itself—such as length, radius (or cross-sectional area), linear mass density, tension, Young's modulus (for stiffness), and damping coefficients—must be adjusted to match those of the specific instrument being synthesized.4 These parameters will vary significantly from violin to double bass.  
  * **Body Resonance Modeling:** Capturing the unique timbral signature of each instrument requires modeling the influence of its body. While a full FDTD or Finite Element Method (FEM) simulation of the instrument body is computationally prohibitive for real-time AudioWorklet applications, several more efficient approximations can be employed:  
    * **Filter Banks:** A bank of parallel resonant filters (e.g., biquad filters) can be used to approximate the most prominent body resonances. The center frequencies, Q-factors (bandwidth), and gains of these filters would need to be carefully tuned for each instrument based on acoustical measurements or known properties.  
    * **Modal Filters:** If the frequencies and damping of the dominant body modes are known, a set of modal resonators can be driven by the string model's output.  
    * **Convolution with Impulse Responses (IRs):** The body's response can be captured in an impulse response. Convolving the output of the string model with a short body IR can impart a realistic resonant character. Measured IRs (as discussed in 34 for violins) or IRs derived from simplified body models could be used. The length of the IR must be kept short to be computationally feasible within AudioWorklet. The parameters for these body approximations (filter settings, modal parameters, choice of IR) become a key area for differentiating the timbres of the violin, viola, cello, and double bass. Research by Viala et al. 35 on violins suggests that geometrical choices in construction (like plate thicknesses and f-hole shapes) have a predominant influence on the instrument's dynamical behavior compared to the natural variability of tonewood material properties. This underscores the importance of accurately representing the geometry-dependent resonant characteristics in the body model.  
* **Perceptual Timbre Space:** Understanding the acoustic features that correlate with perceived timbre dimensions (e.g., brightness, richness, attack quality) can guide the parameter tuning process. Features like spectral centroid (related to brightness), attack time, and spectral flux are known to be important perceptual correlates.36 Adjusting model parameters to target specific values or trajectories for these features can help in crafting distinct and recognizable instrument timbres.

The following table provides a general guide to differentiating physical model parameters for the bowed string family:

| Instrument | Typical String Length Range (approx.) | Relative String Thickness/Mass | Key Body Resonance Freq. Ranges (approx. main wood/air modes) | Primary Timbral Descriptors to Target | Example Model Parameters to Adjust (String & Body Approx.) |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **Violin** | 32-33 cm | Low | A0 (Helmholtz air mode) \~270-290 Hz; Main wood modes (B1-, B1+) \~450-550 Hz | Bright, agile, clear, focused, capable of brilliance | String: length, tension, density for violin E,A,D,G strings. Body filter/IR: Emphasize higher mids, ensure clear separation of A0 and B1 modes. |
| **Viola** | 36-43 cm (variable) | Medium-Low | A0 \~200-240 Hz; Main wood modes \~350-450 Hz | Darker, richer, warmer, more mellow than violin, "throaty" | String: parameters for viola C,G,D,A strings (longer/thicker than violin). Body filter/IR: Lower A0 and wood modes, broader resonances, less prominent high frequencies compared to violin. |
| **Cello** | 68-70 cm | Medium-High | A0 \~100-120 Hz; Main wood modes \~170-220 Hz | Deep, resonant, sonorous, powerful, wide expressive range | String: parameters for cello C,G,D,A strings (significantly longer/thicker). Body filter/IR: Much lower A0 and wood modes, prominent bass response, rich harmonic spectrum. |
| **Double Bass** | 100-115 cm | High | A0 \~55-65 Hz; Main wood modes \~90-120 Hz | Very deep, powerful, weighty, can be "growly" or smooth | String: parameters for bass E,A,D,G strings (longest/thickest). Body filter/IR: Very low A0 and wood modes, strong fundamental emphasis, extended low-frequency response. |

*(Note: Resonance frequencies are approximate and vary considerably between individual instruments.)*

### **C. Simulating Advanced Playing Techniques**

Beyond basic note production, skilled players employ a wide array of techniques to color the sound and articulate musical phrases. Physical models, due to their basis in physics, are often capable of reproducing these techniques if the relevant control parameters and interactions are modeled.

* **Sul Ponticello:** This technique involves bowing very close to the bridge. It produces a characteristically bright, glassy, and sometimes harsh or metallic sound, with a strong emphasis on upper harmonics and often a weaker fundamental. In a physical model, this is achieved by setting the bow position parameter (xb​) to a small value (close to the bridge).37 A well-constructed physical model should naturally exhibit this timbral shift as the bow approaches a termination point where higher modes have larger amplitudes.  
* **Sul Tasto:** The opposite of sul ponticello, sul tasto involves bowing over or near the end of the fingerboard. This results in a softer, rounder, more flute-like (flautando) sound with a stronger fundamental and attenuated upper harmonics. It is modeled by setting xb​ to a larger value (further from the bridge).37  
* **Bow Noise / Rosin Noise:** This refers to the audible "scratchy" or "hissy" component of the sound, often more noticeable at the beginning of notes (attack transient), with light bow pressure, or when the bow speed is slow. This noise arises from the complex, irregular micro-interactions at the bow-string contact point. Advanced friction models, particularly elasto-plastic models that account for bristle dynamics and micro-slips, can inherently generate some of these noise-like components.16 Alternatively, it can be modeled as an additive noise source whose characteristics (e.g., amplitude, spectral content) are modulated by bowing parameters like pressure and velocity.  
* **Spiccato/Sautillé:** These are detached, bouncing bow strokes. Spiccato is typically a controlled off-string stroke, while sautille is a very fast, on-string bouncing that arises more from the bow's natural resilience. Modeling these requires simulating the bow's vertical motion, including collisions with and rebounds from the string. The Hunt and Crossley collision model, as used in Desvages' work for normal force interactions 4, is relevant for capturing the impact dynamics. The NESS model also considers refined control of bow bouncing for spiccato.4  
* **Anomalous Low Frequencies (ALF) / Subharmonics:** Under certain bowing conditions (often with specific combinations of bow force, speed, and position, particularly at large bow-bridge distances), the string can vibrate at frequencies below the normal fundamental, producing subharmonic tones.4 These can include period-doubling (octave below) or other fractional multiples of the fundamental. A sufficiently detailed nonlinear physical model should be capable of reproducing these phenomena as emergent behaviors of the bow-string interaction.  
* **Flageolet (Natural Harmonics):** These are high, flute-like tones produced by lightly touching an open string at a nodal point (e.g., at one-half, one-third, or one-quarter of its length) without fully pressing it against the fingerboard. This damps the fundamental and other modes that do not have a node at that point, allowing the corresponding harmonic to sound prominently. Modeling this involves applying a light finger force at the correct position, which selectively damps certain modes of vibration.21

## **VI. Practical Implementation in AudioWorklet: Recommendations and Best Practices**

Successfully implementing state-of-the-art bowed string synthesis in AudioWorklet requires a strategic approach that balances algorithmic sophistication with the computational and architectural constraints of the web environment.

### **A. Selecting Appropriate Synthesis Models for Web Deployment**

Given the demanding nature of bowed string physical modeling, a phased approach is advisable:

* **Start Simple, Iterate:** Begin by implementing a less computationally intensive model to establish the AudioWorklet pipeline, Wasm integration (if used), and basic control mechanisms. For example, one might start with a single-polarisation digital waveguide model with a simple velocity-dependent friction curve, or a modal synthesis model with a limited number of modes and a basic excitation. This allows for early testing and performance benchmarking in the target environment.  
* **Prioritize Non-Iterative Solutions:** For modeling nonlinearities, especially the crucial bow-string friction, preference should be given to numerical schemes that are non-iterative or have a strictly bounded (and small) number of iterations per time step. Methods like the non-iterative modal solver by Russo et al. 18 are highly advantageous because their per-block computation time is predictable and consistent, which is vital for meeting AudioWorklet's 128-sample processing deadline. Iterative solvers like Newton-Raphson, while powerful, can have variable convergence rates depending on the system state, making them risky for hard real-time applications.  
* **Modular Design:** Even if the entire synthesis algorithm for a single instrument runs within one AudioWorkletProcessor, structuring the code in a modular fashion (e.g., separate C++/Wasm modules for the string dynamics, friction model, body resonance, and control parameter mapping) is highly recommended.18 This approach aids in development, debugging, unit testing, and potential future optimizations or replacements of individual components. It also makes the codebase more manageable and understandable.

### **B. Optimization Strategies for Complex Algorithms in AudioWorklet and Wasm**

To run complex physical models in real time, aggressive optimization is essential:

* **Aggressive Wasm Optimization:** When compiling C/C++ code to WebAssembly using tools like Emscripten, utilize the highest available compiler optimization levels (e.g., \-O3, \-Os for size, Link-Time Optimization (LTO)). Profile the Wasm output to ensure critical loops and functions are performing as expected.  
* **Leverage SIMD:** For algorithms that exhibit data parallelism (common in FDTD schemes involving operations on grid arrays, or in some modal update calculations), employ Wasm SIMD instructions.2 This can provide significant speedups by performing operations on multiple data elements concurrently.  
* **Minimize JS-Wasm Interoperability Overhead:** Calls between JavaScript (running in the AudioWorkletGlobalScope) and Wasm modules incur some overhead. Minimize the frequency of these calls and the amount of data copied. For frequent updates of control parameters from JS to Wasm, consider writing them to a region of Wasm memory (e.g., via SharedArrayBuffer if inter-thread communication is needed from the main thread, or directly if within the same AudioWorkletProcessor) that the Wasm code can then read directly.2  
* **Profile Extensively:** Use browser developer tools (profilers, performance timelines) to meticulously measure the execution time of the process() method in your AudioWorkletProcessor. Identify bottlenecks, whether they are in the JavaScript glue code or within the Wasm module. Pay close attention to ensuring the total execution time per 128-sample block remains well below the \~2.67 ms threshold (at 48 kHz). The consequence of consistently exceeding this budget is not typically a hard crash or explicit error, but rather a degradation of audio quality with glitches, stuttering, or silence.2 This "silent failure" mode makes proactive performance monitoring critical.  
* **Precomputation:** Wherever possible, precompute values that do not change during real-time processing. This can include filter coefficients, lookup tables for nonlinear functions (e.g., friction curves), or parts of matrices used in numerical solvers (as in the non-iterative modal solver where components of the system matrix inverse can be pre-calculated 24).

Optimization must be holistic. Even a highly optimized Wasm core can be bottlenecked by inefficient JavaScript code for data marshalling, excessive memory allocations in the JS part of the AudioWorkletProcessor, or even too much diagnostic console logging during development.1 The entire execution path within the process() method contributes to the time budget.

### **C. Managing Model State and Real-Time Parameter Control**

The AudioWorkletProcessor instance is persistent for the lifetime of its corresponding AudioWorkletNode.

* **State Preservation:** The internal state of the physical model (e.g., string displacements and velocities at all grid points in an FDTD model, modal amplitudes and velocities in a modal model, filter states for body resonance) must be stored as member variables of the AudioWorkletProcessor class (or within its Wasm module's memory) so that it persists between calls to the process() method.  
* **Parameter Updates:**  
  * **AudioParam:** For parameters that need sample-accurate automation and fit the AudioParam model (i.e., floating-point values that can be scheduled for changes over time), these provide an efficient, built-in mechanism.1 The process() method receives an updated array of these parameter values for each block.  
  * **MessagePort:** For more complex control data (e.g., switching friction models, updating string configurations, sending arrays of data) or parameters that are not easily represented by a single float, communication from the main browser thread to the AudioWorkletProcessor can be achieved using the port property (a MessagePort) of the AudioWorkletNode and its processor counterpart.1 Messages posted from the main thread will trigger an onmessage event handler in the AudioWorkletProcessor. Be mindful of the overhead of MessagePort communication; avoid sending very large or very frequent messages if performance is critical.2  
* **Ring Buffers for Asynchronous Data:** When receiving control data from the main thread via MessagePort (which is asynchronous to the audio processing), it's often necessary to use a thread-safe mechanism to transfer this data to the real-time audio processing loop. Lock-free ring buffers (which can be implemented using SharedArrayBuffer for efficient inter-thread communication if the main thread and AudioWorklet need to share data directly, or within the AudioWorkletProcessor's own memory if just buffering messages) are a common solution. This prevents the audio thread from blocking while waiting for new control data and avoids race conditions.

### **D. Balancing Realism, Computational Load, and Latency**

Achieving the "most realistic possible" sound is an ambitious goal that must be tempered by the practicalities of web deployment.

* **Progressive Realism:** Adopt an iterative development strategy. Start with a model that captures the core sound production (e.g., basic Helmholtz motion). Incrementally add features that enhance realism (e.g., string stiffness, frequency-dependent damping, two-polarisation motion, more sophisticated friction models, fingerboard interaction, body resonance). Profile performance at each stage to ensure the computational cost remains manageable.  
* **Adaptive Complexity (Advanced):** For very complex models, one might consider (though this is difficult to implement robustly) adaptive schemes where the model's complexity is dynamically reduced if the system detects high CPU load or imminent deadline misses. This could involve, for example, reducing the number of modes in a modal synthesizer, using a simpler friction model, or reducing the spatial resolution of an FDTD scheme. However, such adaptations can themselves introduce audible artifacts if not handled carefully.  
* **User-Selectable Quality Levels:** A more practical approach is to offer the end-user different quality/performance settings. For example, a "high quality" mode might use the full-complexity model, while a "lower latency" or "CPU-saver" mode might use a simplified version.

### **E. Specific Instrument Considerations in AudioWorklet**

The physical differences between violin, viola, cello, and double bass have implications for computational load in physical models:

* **Violin/Viola:** These instruments have higher fundamental frequencies and produce significant energy in higher harmonics. In FDTD models, accurately propagating these higher frequencies without excessive numerical dispersion (an artifact where different frequencies travel at incorrect speeds on the discrete grid) may require a finer spatial discretization (more grid points for a given string length) or operating at a higher internal sampling rate (oversampling) within the model, both of which increase computational load. Modal models might need a larger number of modes to accurately represent the timbre.  
* **Cello/Double Bass:** These instruments have lower fundamental frequencies and longer strings. This might allow for somewhat coarser FDTD grids or fewer modes to capture the fundamental and lower harmonics. However, their rich overtone structures and the importance of their complex, lower-frequency body resonances still demand significant computational resources. The sheer physical length of double bass strings could lead to very large FDTD models if high-frequency components of the attack or bow noise are to be accurately modeled across the entire string.

The choice of model parameters (string physical constants, body resonance filter settings) will be paramount in differentiating these instruments, as discussed in Section V.B.

## **VII. Conclusion and Future Outlook**

### **Summary of Findings**

The synthesis of highly realistic bowed string instrument sounds (violin, viola, cello, and double bass) within the Web Audio API's AudioWorklet environment presents a sophisticated challenge that lies at the intersection of musical acoustics, advanced DSP, and web engineering. Physical modeling techniques, particularly Finite Difference Time Domain (FDTD) methods with advanced elasto-plastic friction and energy-balanced schemes 5, and non-iterative modal synthesis approaches 24, offer the most promising paths towards achieving high levels of realism. These methods allow for detailed simulation of the complex bow-string interaction, two-polarisation string motion, and the incorporation of gestural control parameters crucial for expressive performance.  
The AudioWorklet framework, by providing a dedicated thread for custom audio processing, is a foundational technology for deploying such computationally intensive algorithms on the web.1 However, its fixed 128-sample processing block imposes a strict real-time budget (\~2.67 ms at 48 kHz), demanding highly optimized algorithms. WebAssembly (Wasm) is critical for implementing the core DSP logic, offering near-native execution speeds for code compiled from C/C++.2 Emerging Wasm capabilities like SIMD 10 and the use of SharedArrayBuffer for efficient data communication 2 further enhance performance potential.  
Despite these advancements, significant hurdles remain. The variation in real-time thread priority for AudioWorklet across different browser implementations (notably the "DISPLAY" priority in Chromium-based browsers 9) can compromise the reliability of demanding audio applications under system load. Furthermore, the sheer computational cost of high-fidelity physical models necessitates careful algorithmic design, aggressive optimization, and a keen awareness of the trade-offs between realism and performance. Hybrid methods, combining physical models with machine learning for control or component approximation 29, and neural synthesis techniques like DDSP 31, show potential but face their own challenges regarding expressive control, computational load in real-time, and the need for extensive, high-quality training data.

### **The Feasibility of High Realism**

Achieving a high degree of bowed string realism in AudioWorklet is progressively becoming more attainable, but it is not a trivial undertaking. It requires expert-level implementation skills, a deep understanding of both the acoustics of the instruments and the intricacies of numerical methods, and a pragmatic approach to balancing desired model complexity against the stringent computational constraints of the web platform. The most successful strategies will likely involve:

* Carefully chosen physical models, prioritizing those with provably stable and non-iterative (or highly efficient iterative) numerical schemes.  
* Aggressive optimization of Wasm code, leveraging SIMD where applicable.  
* Meticulous management of memory and data flow within the AudioWorkletProcessor to avoid garbage collection and minimize overhead.  
* A modular design approach to manage complexity.  
* Effective modeling of instrument-specific parameters (string properties and body resonances) to differentiate the violin, viola, cello, and double bass.

While the "most realistic possible" sound remains a moving target, the tools and techniques available today allow for compelling and expressive bowed string synthesis on the web, far exceeding what was feasible just a few years ago.

### **Future Directions**

The continued evolution of web technologies and synthesis research points towards several promising future directions:

* **Browser AudioWorklet Enhancements:** Greater consistency in real-time thread priority across browser implementations would significantly improve the robustness of demanding audio applications. Further optimizations in the Web Audio API itself could also reduce overhead.  
* **WebAssembly Advancements:** The ongoing development of Wasm, including features like Wasm threads (distinct from Web Workers, potentially allowing finer-grained parallelism within a Wasm module), improved GC integration (for languages compiled to Wasm that might benefit from managed memory for non-critical parts), and further SIMD extensions, will continue to boost performance.  
* **Accessible Training Data and Hybrid Models:** The growth of open, high-quality datasets pairing detailed performance parameters with audio recordings of bowed string instruments could fuel advances in hybrid and neural synthesis, particularly for capturing subtle expressive nuances and complex articulations.  
* **Web-Friendly Libraries and Frameworks:** The development of more specialized libraries and higher-level frameworks tailored for advanced physical modeling and neural synthesis within the AudioWorklet and Wasm ecosystem could lower the barrier to entry for developers and accelerate innovation.  
* **Improved Body Resonance Modeling:** More efficient yet perceptually accurate methods for modeling instrument body resonances within AudioWorklet's constraints will be key to enhancing timbral realism and differentiation.

### **Final Word**

The synthesis of highly realistic bowed string instruments using AudioWorklet represents a significant but ultimately rewarding endeavor. It pushes the boundaries of what is currently achievable in web-based audio, demanding a synergistic combination of expertise in acoustics, digital signal processing, numerical methods, and web engineering. As the web platform continues to mature and computational power increases, the fidelity and expressiveness of virtual bowed string instruments in the browser are poised for continued and exciting advancements.

#### **Works cited**

1. Audio Worklets for Low-Latency Audio Processing \- DEV Community, accessed on June 5, 2025, [https://dev.to/omriluz1/audio-worklets-for-low-latency-audio-processing-3b9p](https://dev.to/omriluz1/audio-worklets-for-low-latency-audio-processing-3b9p)  
2. High Performance Web Audio with AudioWorklet in Firefox \- Mozilla ..., accessed on June 5, 2025, [https://hacks.mozilla.org/2020/05/high-performance-web-audio-with-audioworklet-in-firefox/](https://hacks.mozilla.org/2020/05/high-performance-web-audio-with-audioworklet-in-firefox/)  
3. AudioWorklet \- Web APIs | MDN, accessed on June 5, 2025, [https://developer.mozilla.org/en-US/docs/Web/API/AudioWorklet](https://developer.mozilla.org/en-US/docs/Web/API/AudioWorklet)  
4. NESS \- Next Generation Sound Synthesis \- The NESS Project, accessed on June 5, 2025, [http://www.ness.music.ed.ac.uk/archives/systems/bowed-string-instruments](http://www.ness.music.ed.ac.uk/archives/systems/bowed-string-instruments)  
5. Physical Modelling of the Bowed String and Applications to Sound Synthesis \- Acoustics and Audio Group \- The University of Edinburgh, accessed on June 5, 2025, [https://www.acoustics.ed.ac.uk/wp-content/uploads/Theses/Desvages\_Charlotte\_\_PhDThesis\_UniversityOfEdinburgh\_2018.pdf](https://www.acoustics.ed.ac.uk/wp-content/uploads/Theses/Desvages_Charlotte__PhDThesis_UniversityOfEdinburgh_2018.pdf)  
6. (PDF) Physical Modeling of Nonlinear Player-String Interactions in Bowed String Sound Synthesis Using Finite Difference Methods \- ResearchGate, accessed on June 5, 2025, [https://www.researchgate.net/publication/262730872\_Physical\_Modeling\_of\_Nonlinear\_Player-String\_Interactions\_in\_Bowed\_String\_Sound\_Synthesis\_Using\_Finite\_Difference\_Methods](https://www.researchgate.net/publication/262730872_Physical_Modeling_of_Nonlinear_Player-String_Interactions_in_Bowed_String_Sound_Synthesis_Using_Finite_Difference_Methods)  
7. www.dafx.de, accessed on June 5, 2025, [https://www.dafx.de/paper-archive/2015/DAFx-15\_submission\_71.pdf](https://www.dafx.de/paper-archive/2015/DAFx-15_submission_71.pdf)  
8. Wasm Audio Worklets API — Emscripten 4.0.9-git (dev) documentation, accessed on June 5, 2025, [https://emscripten.org/docs/api\_reference/wasm\_audio\_worklets.html](https://emscripten.org/docs/api_reference/wasm_audio_worklets.html)  
9. Use a real-time priority thread for Audio Worklet operation \[40563687\] \- Chromium, accessed on June 5, 2025, [https://issues.chromium.org/40563687](https://issues.chromium.org/40563687)  
10. The State of WebAssembly – 2024 and 2025 \- Uno Platform, accessed on June 5, 2025, [https://platform.uno/blog/state-of-webassembly-2024-2025/](https://platform.uno/blog/state-of-webassembly-2024-2025/)  
11. Using SIMD with WebAssembly — Emscripten 4.0.9-git (dev) documentation, accessed on June 5, 2025, [https://emscripten.org/docs/porting/simd.html](https://emscripten.org/docs/porting/simd.html)  
12. Synthesizing Bowed Strings: The Violin Family \- Sound On Sound, accessed on June 5, 2025, [https://www.soundonsound.com/techniques/synthesizing-bowed-strings-violin-family](https://www.soundonsound.com/techniques/synthesizing-bowed-strings-violin-family)  
13. (PDF) Physical Modelling of the Bowed String and Applications to ..., accessed on June 5, 2025, [https://www.researchgate.net/publication/325789500\_Physical\_Modelling\_of\_the\_Bowed\_String\_and\_Applications\_to\_Sound\_Synthesis](https://www.researchgate.net/publication/325789500_Physical_Modelling_of_the_Bowed_String_and_Applications_to_Sound_Synthesis)  
14. DAFx Paper Archive \- Browse all papers byBilbao, S., page 4 of 5, oldest first, accessed on June 5, 2025, [https://www.dafx.de/paper-archive/search?author%5B%5D=Bilbao%2C+S.\&p=4\&s=oldest](https://www.dafx.de/paper-archive/search?author%5B%5D=Bilbao,+S.&p=4&s=oldest)  
15. Real-Time Implementation of a Friction Drum Inspired Instrument Using Finite Difference Schemes \- Digital Audio Effects (DAFx), accessed on June 5, 2025, [https://dafx2020.mdw.ac.at/proceedings/papers/DAFx20in21\_paper\_32.pdf](https://dafx2020.mdw.ac.at/proceedings/papers/DAFx20in21_paper_32.pdf)  
16. www.dafx.de, accessed on June 5, 2025, [https://www.dafx.de/paper-archive/2019/DAFx2019\_paper\_18.pdf](https://www.dafx.de/paper-archive/2019/DAFx2019_paper_18.pdf)  
17. www.dafx.de, accessed on June 5, 2025, [https://www.dafx.de/paper-archive/2019/DAFx2019\_paper\_6.pdf](https://www.dafx.de/paper-archive/2019/DAFx2019_paper_6.pdf)  
18. www.dafx.de, accessed on June 5, 2025, [https://www.dafx.de/paper-archive/2019/DAFx2019\_paper\_22.pdf](https://www.dafx.de/paper-archive/2019/DAFx2019_paper_22.pdf)  
19. Two-Polarisation Physical Model of Bowed Strings with Nonlinear Contact and Friction Forces, and Application to Gesture-Based Sound Synthesis \- MDPI, accessed on June 5, 2025, [https://www.mdpi.com/2076-3417/6/5/135](https://www.mdpi.com/2076-3417/6/5/135)  
20. (PDF) Two-Polarisation Physical Model of Bowed Strings with Nonlinear Contact and Friction Forces, and Application to Gesture-Based Sound Synthesis \- ResearchGate, accessed on June 5, 2025, [https://www.researchgate.net/publication/302917594\_Two-Polarisation\_Physical\_Model\_of\_Bowed\_Strings\_with\_Nonlinear\_Contact\_and\_Friction\_Forces\_and\_Application\_to\_Gesture-Based\_Sound\_Synthesis](https://www.researchgate.net/publication/302917594_Two-Polarisation_Physical_Model_of_Bowed_Strings_with_Nonlinear_Contact_and_Friction_Forces_and_Application_to_Gesture-Based_Sound_Synthesis)  
21. Bowed string as a waveguide signal processing scheme ..., accessed on June 5, 2025, [https://www.researchgate.net/figure/Bowed-string-as-a-waveguide-signal-processing-scheme\_fig3\_298976946](https://www.researchgate.net/figure/Bowed-string-as-a-waveguide-signal-processing-scheme_fig3_298976946)  
22. Synthesis of Bowed Strings \- University of Michigan, accessed on June 5, 2025, [https://quod.lib.umich.edu/i/icmc/bbp2372.1982.027/--synthesis-of-bowed-strings?rgn=main;view=fulltext](https://quod.lib.umich.edu/i/icmc/bbp2372.1982.027/--synthesis-of-bowed-strings?rgn=main;view%3Dfulltext)  
23. Banded waveguide synthesis \- Wikipedia, accessed on June 5, 2025, [https://en.wikipedia.org/wiki/Banded\_waveguide\_synthesis](https://en.wikipedia.org/wiki/Banded_waveguide_synthesis)  
24. site.unibo.it, accessed on June 5, 2025, [https://site.unibo.it/nemus-numerical-sound-restoration/en/resources/publications/dafx2022.pdf/@@download/file/DAFx2022.pdf](https://site.unibo.it/nemus-numerical-sound-restoration/en/resources/publications/dafx2022.pdf/@@download/file/DAFx2022.pdf)  
25. Publications — NEMUS \- Numerical Restoration of Historical Musical Instruments \- Unibo \- Università di Bologna, accessed on June 5, 2025, [https://site.unibo.it/nemus-numerical-sound-restoration/en/resources/publications](https://site.unibo.it/nemus-numerical-sound-restoration/en/resources/publications)  
26. accessed on January 1, 1970, [https://dafx.de/paper-archive/2022/DAFx22\_paper\_31.pdf](https://dafx.de/paper-archive/2022/DAFx22_paper_31.pdf)  
27. (PDF) Modular Physical Models in a Real-Time Interactive Application \- ResearchGate, accessed on June 5, 2025, [https://www.researchgate.net/publication/363641364\_Modular\_Physical\_Models\_in\_a\_Real-Time\_Interactive\_Application](https://www.researchgate.net/publication/363641364_Modular_Physical_Models_in_a_Real-Time_Interactive_Application)  
28. Multiple real-time AudioWorklet processing threads? · Issue \#2500 · WebAudio/web-audio-api \- GitHub, accessed on June 5, 2025, [https://github.com/WebAudio/web-audio-api/issues/2500](https://github.com/WebAudio/web-audio-api/issues/2500)  
29. (PDF) Physical modeling meets machine learning: Teaching bow control to a virtual violinist, accessed on June 5, 2025, [https://www.researchgate.net/publication/267236503\_Physical\_modeling\_meets\_machine\_learning\_Teaching\_bow\_control\_to\_a\_virtual\_violinist](https://www.researchgate.net/publication/267236503_Physical_modeling_meets_machine_learning_Teaching_bow_control_to_a_virtual_violinist)  
30. Physics-Informed Deep Learning for Nonlinear Friction Model of Bow-string Interaction, accessed on June 5, 2025, [https://arxiv.org/html/2505.18950v1](https://arxiv.org/html/2505.18950v1)  
31. boblsturm.github.io, accessed on June 5, 2025, [https://boblsturm.github.io/aimusic2020/papers/CSMC\_\_MuMe\_2020\_paper\_29.pdf](https://boblsturm.github.io/aimusic2020/papers/CSMC__MuMe_2020_paper_29.pdf)  
32. (PDF) Real-time Timbre Transfer and Sound Synthesis using DDSP, accessed on June 5, 2025, [https://www.researchgate.net/publication/350061825\_Real-time\_Timbre\_Transfer\_and\_Sound\_Synthesis\_using\_DDSP](https://www.researchgate.net/publication/350061825_Real-time_Timbre_Transfer_and_Sound_Synthesis_using_DDSP)  
33. Compilation of Changes to the CPC Scheme Between 2024.05 and 2024.08 \- USPTO, accessed on June 5, 2025, [https://www.uspto.gov/web/patents/classification/cpc/compilations/cpc-compilation-202408.pdf](https://www.uspto.gov/web/patents/classification/cpc/compilations/cpc-compilation-202408.pdf)  
34. Method for measuring violin sound radiation based on bowed glissandi and its application to sound synthesis | Request PDF \- ResearchGate, accessed on June 5, 2025, [https://www.researchgate.net/publication/51605704\_Method\_for\_measuring\_violin\_sound\_radiation\_based\_on\_bowed\_glissandi\_and\_its\_application\_to\_sound\_synthesis](https://www.researchgate.net/publication/51605704_Method_for_measuring_violin_sound_radiation_based_on_bowed_glissandi_and_its_application_to_sound_synthesis)  
35. (PDF) Model based ranking of the influence of geometry and materials on the dynamical behavior of the violin highlights predominance of geometrical choices \- ResearchGate, accessed on June 5, 2025, [https://www.researchgate.net/publication/386208428\_Model\_based\_ranking\_of\_the\_influence\_of\_geometry\_and\_materials\_on\_the\_dynamical\_behavior\_of\_the\_violin\_highlights\_predominance\_of\_geometrical\_choices](https://www.researchgate.net/publication/386208428_Model_based_ranking_of_the_influence_of_geometry_and_materials_on_the_dynamical_behavior_of_the_violin_highlights_predominance_of_geometrical_choices)  
36. Acoustic structure of the five perceptual dimensions of timbre in orchestral instrument tones, accessed on June 5, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC3548835/](https://pmc.ncbi.nlm.nih.gov/articles/PMC3548835/)  
37. The Science of String Instruments | Request PDF \- ResearchGate, accessed on June 5, 2025, [https://www.researchgate.net/publication/224034909\_The\_Science\_of\_String\_Instruments](https://www.researchgate.net/publication/224034909_The_Science_of_String_Instruments)  
38. (PDF) The Violinist's Sound Palette: Spectral Centroid, Pitch Flattening and Anomalous Low Frequencies \- ResearchGate, accessed on June 5, 2025, [https://www.researchgate.net/publication/233566597\_The\_Violinist's\_Sound\_Palette\_Spectral\_Centroid\_Pitch\_Flattening\_and\_Anomalous\_Low\_Frequencies](https://www.researchgate.net/publication/233566597_The_Violinist's_Sound_Palette_Spectral_Centroid_Pitch_Flattening_and_Anomalous_Low_Frequencies)  
39. accessed on January 1, 1970, [https://arxiv.org/pdf/2401.14007](https://arxiv.org/pdf/2401.14007)  
40. arxiv.org, accessed on June 5, 2025, [https://arxiv.org/pdf/2505.18950v1](https://arxiv.org/pdf/2505.18950v1)